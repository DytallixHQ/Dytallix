name: Cross-Chain Bridge Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'tests/**'
      - 'smart-contracts/**'
      - 'interoperability/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'tests/**'
      - 'smart-contracts/**'
      - 'interoperability/**'
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of test to run'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - basic
          - comprehensive
          - stress
          - security
      enable_monitoring:
        description: 'Enable monitoring during tests'
        required: false
        default: true
        type: boolean

env:
  RUST_BACKTRACE: 1
  CARGO_TERM_COLOR: always

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      test-matrix: ${{ steps.test-matrix.outputs.matrix }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup test matrix
        id: test-matrix
        run: |
          if [ "${{ github.event.inputs.test_type }}" = "basic" ]; then
            echo 'matrix=["basic"]' >> $GITHUB_OUTPUT
          elif [ "${{ github.event.inputs.test_type }}" = "stress" ]; then
            echo 'matrix=["stress", "performance"]' >> $GITHUB_OUTPUT
          elif [ "${{ github.event.inputs.test_type }}" = "security" ]; then
            echo 'matrix=["security", "edge-cases"]' >> $GITHUB_OUTPUT
          else
            echo 'matrix=["basic", "ai-generation", "bidirectional", "stress", "security", "monitoring"]' >> $GITHUB_OUTPUT
          fi

  rust-tests:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix:
        test-suite: ${{ fromJson(needs.setup.outputs.test-matrix) }}

    services:
      ethereum:
        image: trufflesuite/ganache-cli:latest
        ports:
          - 8545:8545
        options: >-
          --health-cmd="curl -f http://localhost:8545 || exit 1"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=3

      cosmos:
        image: cosmwasm/workspace-optimizer:0.12.13
        ports:
          - 26657:26657
        options: >-
          --health-cmd="curl -f http://localhost:26657/status || exit 1"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=3

    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      - name: Cache Rust dependencies
        uses: Swatinem/rust-cache@v2
        with:
          key: ${{ matrix.test-suite }}

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install asyncio dataclasses typing_extensions

      - name: Setup test environment
        run: |
          # Create test data directories
          mkdir -p /tmp/bridge_test_data
          mkdir -p /tmp/monitoring_data

          # Set environment variables for testing
          echo "ETHEREUM_RPC_URL=http://localhost:8545" >> $GITHUB_ENV
          echo "COSMOS_RPC_URL=http://localhost:26657" >> $GITHUB_ENV
          echo "TEST_MODE=ci" >> $GITHUB_ENV

      - name: Run Rust linting
        run: |
          cargo fmt --all -- --check
          cargo clippy --workspace --all-targets --all-features -- -D warnings

      - name: Build test suite
        run: |
          cargo build --workspace --tests --verbose

      - name: Run basic Rust tests
        if: matrix.test-suite == 'basic'
        run: |
          cargo test --workspace --lib test_ai_generator_creation
          cargo test --workspace --lib test_orchestrator_creation
          cargo test --workspace --lib test_monitoring_system_integration
          cargo test --workspace --lib test_cross_chain_flow_simulation

      - name: Run AI generation tests
        if: matrix.test-suite == 'ai-generation'
        run: |
          cargo test --workspace --lib test_ai_test_generator_integration
          cargo test --workspace --lib ai_test_generator::tests

      - name: Run bridge orchestrator tests
        if: matrix.test-suite == 'bidirectional'
        run: |
          cargo test --workspace --lib test_bridge_orchestrator_integration
          cargo test --workspace --lib bridge_orchestrator::tests

      - name: Run stress tests
        if: matrix.test-suite == 'stress'
        timeout-minutes: 30
        run: |
          # Run Python stress test runner
          python tests/src/bin/comprehensive_test_runner.py
        env:
          TEST_TYPE: stress
          MAX_CONCURRENT_TESTS: 10
          STRESS_DURATION: 300

      - name: Run security tests
        if: matrix.test-suite == 'security'
        run: |
          # Run security-focused tests
          cargo test --workspace --lib security
          python tests/src/bin/comprehensive_test_runner.py
        env:
          TEST_TYPE: security
          ENABLE_AI_FRAUD_DETECTION: true

      - name: Run monitoring tests
        if: matrix.test-suite == 'monitoring'
        run: |
          cargo test --workspace --lib monitoring_system::tests
          cargo test --workspace --lib test_monitoring_system_integration

      - name: Generate test coverage
        if: matrix.test-suite == 'basic'
        run: |
          cargo install cargo-tarpaulin || true
          cargo tarpaulin --workspace --out xml --output-dir ./coverage

      - name: Upload coverage to Codecov
        if: matrix.test-suite == 'basic'
        uses: codecov/codecov-action@v5
        with:
          file: ./coverage/cobertura.xml
          fail_ci_if_error: false

  comprehensive-integration-tests:
    needs: rust-tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'comprehensive' || github.event_name == 'schedule'

    services:
      ethereum:
        image: trufflesuite/ganache-cli:latest
        ports:
          - 8545:8545
        options: >-
          --health-cmd="curl -f http://localhost:8545 || exit 1"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=3

      cosmos:
        image: cosmwasm/workspace-optimizer:0.12.13
        ports:
          - 26657:26657
        options: >-
          --health-cmd="curl -f http://localhost:26657/status || exit 1"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=3

      redis:
        image: redis:alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd="redis-cli ping"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=3

    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache Rust dependencies
        uses: Swatinem/rust-cache@v2

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install asyncio dataclasses typing_extensions pytest pytest-asyncio

      - name: Setup comprehensive test environment
        run: |
          # Create comprehensive test environment
          mkdir -p /tmp/comprehensive_test_data
          mkdir -p /tmp/test_reports
          mkdir -p /tmp/monitoring_logs

          # Setup test configuration
          cat > /tmp/test_config.json << EOF
          {
            "ethereum_rpc_url": "http://localhost:8545",
            "cosmos_rpc_url": "http://localhost:26657",
            "redis_url": "redis://localhost:6379",
            "enable_ai_generation": true,
            "enable_bidirectional_tests": true,
            "enable_stress_tests": true,
            "enable_monitoring": true,
            "max_concurrent_tests": 8,
            "test_timeout_seconds": 600
          }
          EOF

      - name: Run comprehensive bridge tests
        timeout-minutes: 45
        run: |
          echo "üåâ Starting comprehensive cross-chain bridge tests..."

          # Run the comprehensive test suite
          python tests/src/bin/comprehensive_test_runner.py

          # Run Rust integration tests
          cargo test --workspace test_run_cross_chain_tests --release
        env:
          TEST_CONFIG_FILE: /tmp/test_config.json
          ENABLE_COMPREHENSIVE_LOGGING: true
          RUST_LOG: info

      - name: Analyze test results
        if: always()
        run: |
          echo "üìä Analyzing test results..."

          # Check if test reports exist
          if [ -f "/tmp/bridge_test_comprehensive_report.json" ]; then
            echo "‚úÖ Comprehensive test report found"

            # Extract key metrics
            python3 -c "
          import json

          try:
              with open('/tmp/bridge_test_comprehensive_report.json', 'r') as f:
                  report = json.load(f)

              total_tests = report.get('total_tests', 0)
              successful_tests = report.get('successful_tests', 0)
              failed_tests = report.get('failed_tests', 0)

              success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0

              print(f'Total Tests: {total_tests}')
              print(f'Successful: {successful_tests}')
              print(f'Failed: {failed_tests}')
              print(f'Success Rate: {success_rate:.2f}%')

              # Set outputs for GitHub
              with open('/tmp/test_summary.txt', 'w') as f:
                  f.write(f'total_tests={total_tests}\n')
                  f.write(f'successful_tests={successful_tests}\n')
                  f.write(f'failed_tests={failed_tests}\n')
                  f.write(f'success_rate={success_rate:.2f}\n')

          except Exception as e:
              print(f'Error analyzing results: {e}')
              with open('/tmp/test_summary.txt', 'w') as f:
                  f.write('total_tests=0\n')
                  f.write('successful_tests=0\n')
                  f.write('failed_tests=1\n')
                  f.write('success_rate=0.0\n')
          "
          else
            echo "‚ùå Test report not found"
            echo "total_tests=0" > /tmp/test_summary.txt
            echo "successful_tests=0" >> /tmp/test_summary.txt
            echo "failed_tests=1" >> /tmp/test_summary.txt
            echo "success_rate=0.0" >> /tmp/test_summary.txt
          fi

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: comprehensive-test-results
          path: |
            /tmp/bridge_test_comprehensive_report.json
            /tmp/bridge_test_results.log
            /tmp/test_summary.txt
            /tmp/monitoring_logs/
          retention-days: 30

      - name: Post test summary to PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');

            let summary = "## üåâ Cross-Chain Bridge Test Results\n\n";

            try {
              const summaryData = fs.readFileSync('/tmp/test_summary.txt', 'utf8');
              const lines = summaryData.split('\n').filter(line => line.trim());
              const data = {};

              lines.forEach(line => {
                const [key, value] = line.split('=');
                data[key] = value;
              });

              const successRate = parseFloat(data.success_rate || 0);
              const statusEmoji = successRate >= 95 ? "üéâ" : successRate >= 85 ? "‚úÖ" : successRate >= 70 ? "‚ö†Ô∏è" : "‚ùå";

              summary += `${statusEmoji} **Overall Status**: ${successRate >= 85 ? "PASSED" : "NEEDS ATTENTION"}\n\n`;
              summary += `üìä **Test Statistics**:\n`;
              summary += `- Total Tests: ${data.total_tests || 0}\n`;
              summary += `- Successful: ${data.successful_tests || 0}\n`;
              summary += `- Failed: ${data.failed_tests || 0}\n`;
              summary += `- Success Rate: ${data.success_rate || 0}%\n\n`;

              if (successRate >= 95) {
                summary += "üéâ **Excellent!** Bridge system is highly reliable and ready for production.\n";
              } else if (successRate >= 85) {
                summary += "‚úÖ **Good!** Bridge system is reliable with minor issues to address.\n";
              } else if (successRate >= 70) {
                summary += "‚ö†Ô∏è **Acceptable** Bridge system needs improvement before production.\n";
              } else {
                summary += "‚ùå **Needs Work** Bridge system requires significant improvements.\n";
              }

            } catch (error) {
              summary += `‚ùå **Error**: Failed to analyze test results - ${error.message}\n`;
            }

            // Post comment on PR
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

      - name: Check test success rate
        if: always()
        run: |
          if [ -f "/tmp/test_summary.txt" ]; then
            source /tmp/test_summary.txt
            echo "Success rate: $success_rate%"

            # Fail the job if success rate is below threshold
            if (( $(echo "$success_rate < 80.0" | bc -l) )); then
              echo "‚ùå Test success rate ($success_rate%) is below minimum threshold (80%)"
              exit 1
            else
              echo "‚úÖ Test success rate ($success_rate%) meets minimum threshold"
            fi
          else
            echo "‚ùå Test summary not found"
            exit 1
          fi

  performance-analysis:
    needs: comprehensive-integration-tests
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'comprehensive'

    steps:
      - uses: actions/checkout@v4

      - name: Download test artifacts
        uses: actions/download-artifact@v3
        with:
          name: comprehensive-test-results
          path: ./test-results

      - name: Analyze performance trends
        run: |
          echo "üìà Analyzing performance trends..."

          # This would typically analyze performance data over time
          # and generate trend reports

          if [ -f "./test-results/bridge_test_comprehensive_report.json" ]; then
            echo "‚úÖ Performance analysis data available"

            # Extract performance metrics (simplified example)
            python3 -c "
          import json

          try:
              with open('./test-results/bridge_test_comprehensive_report.json', 'r') as f:
                  report = json.load(f)

              monitoring_data = report.get('monitoring_data', {})
              performance_benchmarks = monitoring_data.get('performance_benchmarks', {})

              print('Performance Analysis:')
              for benchmark, metrics in performance_benchmarks.items():
                  print(f'  {benchmark}: {metrics}')

          except Exception as e:
              print(f'Error analyzing performance: {e}')
          "
          else
            echo "‚ö†Ô∏è Performance data not available"
          fi

      - name: Generate performance badge
        run: |
          # Generate a performance badge for the README
          echo "Generating performance badge..."

          # This would create a badge showing current performance metrics
          # For now, just create a placeholder
          echo "![Bridge Performance](https://img.shields.io/badge/Bridge%20Performance-Excellent-green)" > performance-badge.md

  notification:
    needs: [rust-tests, comprehensive-integration-tests]
    runs-on: ubuntu-latest
    if: always() && (github.event_name == 'schedule' || github.ref == 'refs/heads/main')

    steps:
      - name: Notify on test completion
        uses: actions/github-script@v6
        with:
          script: |
            const needs = ${{ toJson(needs) }};

            let allSuccessful = true;
            let summary = "## üåâ Dytallix Cross-Chain Bridge - Test Suite Results\n\n";

            for (const [jobName, jobResult] of Object.entries(needs)) {
              const status = jobResult.result === 'success' ? '‚úÖ' : '‚ùå';
              summary += `${status} **${jobName}**: ${jobResult.result}\n`;

              if (jobResult.result !== 'success') {
                allSuccessful = false;
              }
            }

            const overallStatus = allSuccessful ? "üéâ ALL TESTS PASSED" : "‚ö†Ô∏è SOME TESTS FAILED";
            summary = `# ${overallStatus}\n\n` + summary;

            summary += `\n**Timestamp**: ${new Date().toISOString()}\n`;
            summary += `**Branch**: ${context.ref}\n`;
            summary += `**Commit**: ${context.sha.substring(0, 7)}\n`;

            console.log(summary);

            // In a real implementation, you might send this to Slack, Discord, etc.
            // For now, we'll just log it