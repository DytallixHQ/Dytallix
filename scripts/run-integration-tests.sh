#!/bin/bash

# =============================================================================
# DYTALLIX TESTNET INTEGRATION TEST SUITE
# =============================================================================
# 
# End-to-end integration testing for deployed testnet that validates all
# components are working together correctly. Tests smart contract deployment,
# transaction processing, consensus mechanism, API functionality, and
# cross-component interactions.
#
# Features:
# - Smart contract deployment and execution testing
# - Transaction processing and validation
# - Consensus mechanism verification
# - API endpoint comprehensive testing
# - WebSocket connection testing
# - Performance validation under load
# - Cross-node consistency verification
# - Monitoring stack integration testing
# - End-to-end workflow validation
#
# =============================================================================

set -euo pipefail

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
LOG_DIR="$PROJECT_ROOT/logs"
TEST_RESULTS_DIR="$PROJECT_ROOT/test-results"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
TEST_LOG="$LOG_DIR/integration_tests_${TIMESTAMP}.log"
TEST_REPORT="$TEST_RESULTS_DIR/integration_report_${TIMESTAMP}.json"

# Node configuration
NODES=("dytallix-node-1" "dytallix-node-2" "dytallix-node-3")
NODE_PORTS=(3030 3032 3034)
HEALTH_PORTS=(8081 8083 8085)
METRICS_PORTS=(9090 9091 9092)

# Test configuration
TEST_TIMEOUT=30
MAX_RETRIES=3
RETRY_DELAY=5
LOAD_TEST_DURATION=60
LOAD_TEST_THREADS=5
LOAD_TEST_TPS_TARGET=100

# Performance expectations
EXPECTED_BLOCK_TIME_MAX=5  # seconds
EXPECTED_TPS_MIN=50        # transactions per second
EXPECTED_AVAILABILITY_MIN=95  # percentage

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Test result tracking
TESTS_TOTAL=0
TESTS_PASSED=0
TESTS_FAILED=0
TESTS_SKIPPED=0

# Logging functions
log() {
    local level="$1"
    shift
    local message="$*"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    echo -e "${level}[${timestamp}]${NC} $message" | tee -a "$TEST_LOG"
}

log_info() {
    log "${GREEN}[INFO]" "$@"
}

log_warn() {
    log "${YELLOW}[WARN]" "$@"
}

log_error() {
    log "${RED}[ERROR]" "$@"
}

log_step() {
    log "${BLUE}[STEP]" "$@"
}

log_success() {
    log "${GREEN}[SUCCESS]" "$@"
}

log_debug() {
    if [[ "${DEBUG:-false}" == "true" ]]; then
        log "${PURPLE}[DEBUG]" "$@"
    fi
}

# Test result functions
test_start() {
    local test_name="$1"
    TESTS_TOTAL=$((TESTS_TOTAL + 1))
    log_step "Starting test: $test_name"
}

test_pass() {
    local test_name="$1"
    TESTS_PASSED=$((TESTS_PASSED + 1))
    log_success "‚úÖ PASS: $test_name"
}

test_fail() {
    local test_name="$1"
    local reason="${2:-Unknown failure}"
    TESTS_FAILED=$((TESTS_FAILED + 1))
    log_error "‚ùå FAIL: $test_name - $reason"
}

test_skip() {
    local test_name="$1"
    local reason="${2:-Test skipped}"
    TESTS_SKIPPED=$((TESTS_SKIPPED + 1))
    log_warn "‚è≠Ô∏è  SKIP: $test_name - $reason"
}

# Utility functions
setup_test_environment() {
    log_step "Setting up integration test environment..."
    
    # Create test directories
    mkdir -p "$LOG_DIR"
    mkdir -p "$TEST_RESULTS_DIR"
    mkdir -p "$TEST_RESULTS_DIR/artifacts"
    
    # Initialize test log
    echo "# Dytallix Integration Test Suite - Started at $(date -Iseconds)" > "$TEST_LOG"
    echo "# Test Session ID: integration-tests-${TIMESTAMP}" >> "$TEST_LOG"
    
    # Initialize test counters
    TESTS_TOTAL=0
    TESTS_PASSED=0
    TESTS_FAILED=0
    TESTS_SKIPPED=0
    
    log_success "Test environment setup completed"
}

retry_test() {
    local max_attempts="$1"
    local delay="$2"
    shift 2
    local cmd="$@"
    
    for attempt in $(seq 1 $max_attempts); do
        if eval "$cmd"; then
            return 0
        else
            if [[ $attempt -lt $max_attempts ]]; then
                log_debug "Test attempt $attempt failed, retrying in ${delay}s..."
                sleep "$delay"
            fi
        fi
    done
    
    return 1
}

# Basic connectivity tests
test_node_connectivity() {
    test_start "Node Connectivity"
    
    local failed_nodes=()
    
    for i in "${!NODES[@]}"; do
        local node="${NODES[$i]}"
        local port="${HEALTH_PORTS[$i]}"
        local health_url="http://localhost:$port/health"
        
        log_debug "Testing connectivity to $node at $health_url"
        
        if retry_test $MAX_RETRIES $RETRY_DELAY "curl -sf --max-time $TEST_TIMEOUT '$health_url' > /dev/null"; then
            log_debug "‚úÖ $node: Connected successfully"
        else
            failed_nodes+=("$node")
            log_debug "‚ùå $node: Connection failed"
        fi
    done
    
    if [[ ${#failed_nodes[@]} -eq 0 ]]; then
        test_pass "Node Connectivity"
    else
        test_fail "Node Connectivity" "Failed nodes: ${failed_nodes[*]}"
    fi
}

# API endpoint comprehensive testing
test_api_endpoints() {
    test_start "API Endpoints Comprehensive Testing"
    
    local endpoints=(
        "/health:GET"
        "/node/info:GET"
        "/node/id:GET"
        "/blockchain/height:GET"
        "/blockchain/stats:GET"
        "/consensus/status:GET"
    )
    
    local failed_tests=()
    local total_endpoint_tests=0
    local passed_endpoint_tests=0
    
    for i in "${!NODES[@]}"; do
        local node="${NODES[$i]}"
        local port="${NODE_PORTS[$i]}"
        
        for endpoint_method in "${endpoints[@]}"; do
            local endpoint=$(echo "$endpoint_method" | cut -d: -f1)
            local method=$(echo "$endpoint_method" | cut -d: -f2)
            local url="http://localhost:$port$endpoint"
            
            total_endpoint_tests=$((total_endpoint_tests + 1))
            
            log_debug "Testing $method $url"
            
            if curl -sf --max-time $TEST_TIMEOUT -X "$method" "$url" > /dev/null; then
                passed_endpoint_tests=$((passed_endpoint_tests + 1))
                log_debug "‚úÖ $node$endpoint: $method request successful"
                
                # Validate JSON response for GET requests
                if [[ "$method" == "GET" ]]; then
                    local response=$(curl -sf --max-time $TEST_TIMEOUT "$url" 2>/dev/null)
                    
                    if echo "$response" | jq . > /dev/null 2>&1; then
                        log_debug "‚úÖ $node$endpoint: Valid JSON response"
                    else
                        log_debug "‚ö†Ô∏è  $node$endpoint: Non-JSON response"
                    fi
                fi
            else
                failed_tests+=("$node$endpoint:$method")
                log_debug "‚ùå $node$endpoint: $method request failed"
            fi
        done
    done
    
    local success_rate=$(( (passed_endpoint_tests * 100) / total_endpoint_tests ))
    
    if [[ $success_rate -ge 90 ]]; then
        test_pass "API Endpoints Comprehensive Testing" 
        log_info "API test success rate: $success_rate% ($passed_endpoint_tests/$total_endpoint_tests)"
    else
        test_fail "API Endpoints Comprehensive Testing" "Success rate too low: $success_rate% ($passed_endpoint_tests/$total_endpoint_tests)"
    fi
}

# Blockchain state consistency testing
test_blockchain_consistency() {
    test_start "Blockchain State Consistency"
    
    local heights=()
    local node_states=()
    local failed_nodes=()
    
    # Collect current state from all nodes
    for i in "${!NODES[@]}"; do
        local node="${NODES[$i]}"
        local port="${NODE_PORTS[$i]}"
        local height_url="http://localhost:$port/blockchain/height"
        local stats_url="http://localhost:$port/blockchain/stats"
        
        log_debug "Collecting state from $node"
        
        if curl -sf --max-time $TEST_TIMEOUT "$height_url" > /dev/null; then
            local height_response=$(curl -sf --max-time $TEST_TIMEOUT "$height_url" 2>/dev/null)
            local stats_response=$(curl -sf --max-time $TEST_TIMEOUT "$stats_url" 2>/dev/null)
            
            if echo "$height_response" | jq . > /dev/null 2>&1; then
                local height=$(echo "$height_response" | jq -r '.height // 0')
                heights[$i]=$height
                
                local node_state="{
                    \"node\": \"$node\",
                    \"height\": $height,
                    \"timestamp\": \"$(date -Iseconds)\"
                }"
                
                if echo "$stats_response" | jq . > /dev/null 2>&1; then
                    local total_tx=$(echo "$stats_response" | jq -r '.total_transactions // 0')
                    node_state=$(echo "$node_state" | jq ".total_transactions = $total_tx")
                fi
                
                node_states[$i]="$node_state"
                
                log_debug "‚úÖ $node: Height=$height"
            else
                failed_nodes+=("$node")
                log_debug "‚ùå $node: Invalid height response"
            fi
        else
            failed_nodes+=("$node")
            log_debug "‚ùå $node: Failed to get height"
        fi
    done
    
    # Analyze consistency
    if [[ ${#failed_nodes[@]} -eq 0 ]]; then
        local min_height=${heights[0]}
        local max_height=${heights[0]}
        
        for height in "${heights[@]}"; do
            if [[ $height -lt $min_height ]]; then
                min_height=$height
            fi
            if [[ $height -gt $max_height ]]; then
                max_height=$height
            fi
        done
        
        local height_diff=$((max_height - min_height))
        
        # Allow small differences due to timing
        if [[ $height_diff -le 2 ]]; then
            test_pass "Blockchain State Consistency"
            log_info "Height range: $min_height - $max_height (diff: $height_diff)"
        else
            test_fail "Blockchain State Consistency" "Height difference too large: $height_diff blocks"
        fi
    else
        test_fail "Blockchain State Consistency" "Failed to get state from nodes: ${failed_nodes[*]}"
    fi
}

# Smart contract deployment testing
test_smart_contract_deployment() {
    test_start "Smart Contract Deployment"
    
    # This is a simplified test - in a real implementation, you'd use the actual CLI
    local contract_test_failed=false
    local deployment_node="${NODES[0]}"
    local deployment_port="${NODE_PORTS[0]}"
    
    log_debug "Testing smart contract deployment capability on $deployment_node"
    
    # Test if smart contract endpoints are available
    local contracts_url="http://localhost:$deployment_port/contracts"
    
    if curl -sf --max-time $TEST_TIMEOUT "$contracts_url" > /dev/null; then
        log_debug "‚úÖ Smart contract endpoints available"
        
        # Try to get contract list
        local contracts_response=$(curl -sf --max-time $TEST_TIMEOUT "$contracts_url" 2>/dev/null)
        
        if echo "$contracts_response" | jq . > /dev/null 2>&1; then
            log_debug "‚úÖ Smart contract list retrieved successfully"
            
            # Test contract deployment simulation
            local deploy_url="http://localhost:$deployment_port/contracts/deploy"
            
            # Create a simple test contract payload
            local test_contract='{
                "name": "TestContract",
                "code": "contract TestContract { function test() public pure returns (string) { return \"Hello World\"; } }",
                "test": true
            }'
            
            # This would normally be a POST request with the contract
            # For this test, we'll just check if the endpoint accepts requests
            if curl -sf --max-time $TEST_TIMEOUT -H "Content-Type: application/json" "$deploy_url" > /dev/null 2>&1; then
                log_debug "‚úÖ Contract deployment endpoint responsive"
            else
                log_debug "‚ö†Ô∏è  Contract deployment endpoint not available (may be expected)"
            fi
        else
            log_debug "‚ö†Ô∏è  Invalid contracts response format"
        fi
        
        test_pass "Smart Contract Deployment"
    else
        # If contracts endpoint is not available, check if this is expected
        log_debug "‚ö†Ô∏è  Smart contract endpoints not available"
        
        # Check if this is a known limitation
        local node_info_url="http://localhost:$deployment_port/node/info"
        
        if curl -sf --max-time $TEST_TIMEOUT "$node_info_url" > /dev/null; then
            local node_info=$(curl -sf --max-time $TEST_TIMEOUT "$node_info_url" 2>/dev/null)
            
            if echo "$node_info" | jq -e '.features.smart_contracts' > /dev/null 2>&1; then
                local smart_contracts_enabled=$(echo "$node_info" | jq -r '.features.smart_contracts')
                
                if [[ "$smart_contracts_enabled" == "false" ]]; then
                    test_skip "Smart Contract Deployment" "Smart contracts not enabled in this build"
                else
                    test_fail "Smart Contract Deployment" "Smart contracts enabled but endpoints not accessible"
                fi
            else
                test_skip "Smart Contract Deployment" "Smart contract feature status unknown"
            fi
        else
            test_fail "Smart Contract Deployment" "Cannot determine smart contract capabilities"
        fi
    fi
}

# Transaction processing testing
test_transaction_processing() {
    test_start "Transaction Processing"
    
    local transaction_test_failed=false
    local test_node="${NODES[0]}"
    local test_port="${NODE_PORTS[0]}"
    
    log_debug "Testing transaction processing on $test_node"
    
    # Get initial transaction count
    local stats_url="http://localhost:$test_port/blockchain/stats"
    local initial_tx_count=0
    
    if curl -sf --max-time $TEST_TIMEOUT "$stats_url" > /dev/null; then
        local stats_response=$(curl -sf --max-time $TEST_TIMEOUT "$stats_url" 2>/dev/null)
        
        if echo "$stats_response" | jq . > /dev/null 2>&1; then
            initial_tx_count=$(echo "$stats_response" | jq -r '.total_transactions // 0')
            log_debug "Initial transaction count: $initial_tx_count"
        fi
    fi
    
    # Test transaction submission endpoint
    local tx_url="http://localhost:$test_port/transactions"
    
    if curl -sf --max-time $TEST_TIMEOUT "$tx_url" > /dev/null; then
        log_debug "‚úÖ Transaction endpoint accessible"
        
        # Try to submit a test transaction (simulation)
        local test_transaction='{
            "to": "test_address_12345",
            "amount": 1,
            "data": "test transaction",
            "test": true
        }'
        
        # Note: This would normally submit a real transaction
        # For this test, we'll check endpoint availability
        local submit_url="http://localhost:$test_port/transactions/submit"
        
        if curl -sf --max-time $TEST_TIMEOUT -H "Content-Type: application/json" "$submit_url" > /dev/null 2>&1; then
            log_debug "‚úÖ Transaction submission endpoint responsive"
            
            # Wait a moment and check if transaction count increased
            sleep 5
            
            if curl -sf --max-time $TEST_TIMEOUT "$stats_url" > /dev/null; then
                local new_stats_response=$(curl -sf --max-time $TEST_TIMEOUT "$stats_url" 2>/dev/null)
                
                if echo "$new_stats_response" | jq . > /dev/null 2>&1; then
                    local new_tx_count=$(echo "$new_stats_response" | jq -r '.total_transactions // 0')
                    
                    if [[ $new_tx_count -ge $initial_tx_count ]]; then
                        log_debug "‚úÖ Transaction processing verified (count: $initial_tx_count ‚Üí $new_tx_count)"
                        test_pass "Transaction Processing"
                    else
                        test_skip "Transaction Processing" "Transaction count unchanged (may be expected in test mode)"
                    fi
                else
                    test_fail "Transaction Processing" "Invalid stats response after transaction test"
                fi
            else
                test_fail "Transaction Processing" "Cannot verify transaction processing"
            fi
        else
            test_skip "Transaction Processing" "Transaction submission endpoint not available"
        fi
    else
        test_skip "Transaction Processing" "Transaction endpoints not available"
    fi
}

# Consensus mechanism testing
test_consensus_mechanism() {
    test_start "Consensus Mechanism"
    
    local consensus_nodes=0
    local total_nodes=${#NODES[@]}
    local failed_consensus_checks=()
    
    for i in "${!NODES[@]}"; do
        local node="${NODES[$i]}"
        local port="${NODE_PORTS[$i]}"
        local consensus_url="http://localhost:$port/consensus/status"
        
        log_debug "Checking consensus status for $node"
        
        if curl -sf --max-time $TEST_TIMEOUT "$consensus_url" > /dev/null; then
            local consensus_response=$(curl -sf --max-time $TEST_TIMEOUT "$consensus_url" 2>/dev/null)
            
            if echo "$consensus_response" | jq . > /dev/null 2>&1; then
                local is_validator=$(echo "$consensus_response" | jq -r '.is_validator // false')
                local consensus_height=$(echo "$consensus_response" | jq -r '.height // 0')
                local peer_count=$(echo "$consensus_response" | jq -r '.peer_count // 0')
                
                if [[ "$is_validator" == "true" ]]; then
                    consensus_nodes=$((consensus_nodes + 1))
                    log_debug "‚úÖ $node: Active validator (Height: $consensus_height, Peers: $peer_count)"
                else
                    log_debug "‚ö†Ô∏è  $node: Not participating as validator"
                fi
            else
                failed_consensus_checks+=("$node")
                log_debug "‚ùå $node: Invalid consensus response"
            fi
        else
            failed_consensus_checks+=("$node")
            log_debug "‚ùå $node: Consensus endpoint not available"
        fi
    done
    
    # Evaluate consensus health
    local min_validators=$(( (total_nodes * 2) / 3 + 1 ))  # 2/3 + 1 for Byzantine fault tolerance
    
    if [[ $consensus_nodes -ge $min_validators ]]; then
        test_pass "Consensus Mechanism"
        log_info "Consensus active with $consensus_nodes/$total_nodes validators (min required: $min_validators)"
    elif [[ $consensus_nodes -gt 0 ]]; then
        test_fail "Consensus Mechanism" "Insufficient validators: $consensus_nodes/$total_nodes (min required: $min_validators)"
    else
        test_fail "Consensus Mechanism" "No active validators found"
    fi
}

# Block production testing
test_block_production() {
    test_start "Block Production"
    
    local initial_heights=()
    local final_heights=()
    local block_production_wait=30  # seconds
    
    # Get initial block heights
    for i in "${!NODES[@]}"; do
        local node="${NODES[$i]}"
        local port="${NODE_PORTS[$i]}"
        local height_url="http://localhost:$port/blockchain/height"
        
        if curl -sf --max-time $TEST_TIMEOUT "$height_url" > /dev/null; then
            local height_response=$(curl -sf --max-time $TEST_TIMEOUT "$height_url" 2>/dev/null)
            
            if echo "$height_response" | jq . > /dev/null 2>&1; then
                local height=$(echo "$height_response" | jq -r '.height // 0')
                initial_heights[$i]=$height
                log_debug "$node initial height: $height"
            else
                initial_heights[$i]=0
            fi
        else
            initial_heights[$i]=0
        fi
    done
    
    log_info "Waiting ${block_production_wait}s for block production..."
    sleep $block_production_wait
    
    # Get final block heights
    local blocks_produced=false
    local total_blocks_produced=0
    
    for i in "${!NODES[@]}"; do
        local node="${NODES[$i]}"
        local port="${NODE_PORTS[$i]}"
        local height_url="http://localhost:$port/blockchain/height"
        
        if curl -sf --max-time $TEST_TIMEOUT "$height_url" > /dev/null; then
            local height_response=$(curl -sf --max-time $TEST_TIMEOUT "$height_url" 2>/dev/null)
            
            if echo "$height_response" | jq . > /dev/null 2>&1; then
                local height=$(echo "$height_response" | jq -r '.height // 0')
                final_heights[$i]=$height
                
                local initial_height=${initial_heights[$i]:-0}
                local blocks_diff=$((height - initial_height))
                
                if [[ $blocks_diff -gt 0 ]]; then
                    blocks_produced=true
                    total_blocks_produced=$((total_blocks_produced + blocks_diff))
                    log_debug "‚úÖ $node: Produced $blocks_diff blocks (height: $initial_height ‚Üí $height)"
                else
                    log_debug "‚ö†Ô∏è  $node: No new blocks produced (height: $height)"
                fi
            fi
        fi
    done
    
    if [[ $blocks_produced == true ]]; then
        local avg_block_time=$(( block_production_wait / (total_blocks_produced > 0 ? total_blocks_produced : 1) ))
        
        if [[ $avg_block_time -le $EXPECTED_BLOCK_TIME_MAX ]]; then
            test_pass "Block Production"
            log_info "Block production active: $total_blocks_produced blocks in ${block_production_wait}s (avg: ${avg_block_time}s/block)"
        else
            test_fail "Block Production" "Block production too slow: ${avg_block_time}s per block (max expected: ${EXPECTED_BLOCK_TIME_MAX}s)"
        fi
    else
        test_fail "Block Production" "No block production detected in ${block_production_wait}s"
    fi
}

# Performance testing under load
test_performance_under_load() {
    test_start "Performance Under Load"
    
    log_info "Running load test: $LOAD_TEST_THREADS threads for ${LOAD_TEST_DURATION}s"
    
    # Create load test script
    local load_test_script="/tmp/dytallix_integration_load_test.sh"
    cat > "$load_test_script" << 'EOF'
#!/bin/bash
thread_id=$1
duration=$2
target_tps=$3
base_ports=($4 $5 $6)

end_time=$(($(date +%s) + duration))
requests=0
successful=0
failed=0
total_response_time=0

while [[ $(date +%s) -lt $end_time ]]; do
    port=${base_ports[$((RANDOM % ${#base_ports[@]}))]}
    url="http://localhost:$port/health"
    
    start_time=$(date +%s%3N)
    if curl -sf --max-time 5 "$url" > /dev/null 2>&1; then
        end_time_ms=$(date +%s%3N)
        response_time=$((end_time_ms - start_time))
        total_response_time=$((total_response_time + response_time))
        successful=$((successful + 1))
    else
        failed=$((failed + 1))
    fi
    
    requests=$((requests + 1))
    
    # Control request rate
    sleep_time=$(echo "scale=3; 1 / $target_tps" | bc -l 2>/dev/null || echo "0.01")
    sleep "$sleep_time" 2>/dev/null || sleep 0.01
done

avg_response_time=0
if [[ $successful -gt 0 ]]; then
    avg_response_time=$((total_response_time / successful))
fi

achieved_tps=$(echo "scale=2; $requests / $duration" | bc -l 2>/dev/null || echo "0")

echo "Thread $thread_id: $requests requests, $successful successful, $failed failed"
echo "Thread $thread_id: avg_response_time=${avg_response_time}ms, tps=$achieved_tps"
EOF

    chmod +x "$load_test_script"
    
    # Run load test
    local pids=()
    local thread_tps=$((LOAD_TEST_TPS_TARGET / LOAD_TEST_THREADS))
    
    for ((thread=1; thread<=LOAD_TEST_THREADS; thread++)); do
        "$load_test_script" "$thread" "$LOAD_TEST_DURATION" "$thread_tps" "${NODE_PORTS[@]}" &
        pids+=($!)
    done
    
    # Wait for all threads to complete
    for pid in "${pids[@]}"; do
        wait "$pid"
    done
    
    # Collect results (simplified - in real implementation, you'd parse thread outputs)
    local load_test_successful=true
    
    # Check if nodes are still responsive after load test
    local responsive_nodes=0
    
    for i in "${!NODES[@]}"; do
        local node="${NODES[$i]}"
        local port="${HEALTH_PORTS[$i]}"
        local health_url="http://localhost:$port/health"
        
        if curl -sf --max-time $TEST_TIMEOUT "$health_url" > /dev/null; then
            responsive_nodes=$((responsive_nodes + 1))
        fi
    done
    
    # Clean up
    rm -f "$load_test_script"
    
    # Evaluate results
    local responsiveness_rate=$(( (responsive_nodes * 100) / ${#NODES[@]} ))
    
    if [[ $responsiveness_rate -ge $EXPECTED_AVAILABILITY_MIN ]]; then
        test_pass "Performance Under Load"
        log_info "Load test completed: $responsive_nodes/${#NODES[@]} nodes responsive ($responsiveness_rate%)"
    else
        test_fail "Performance Under Load" "Node responsiveness too low after load test: $responsiveness_rate%"
    fi
}

# Monitoring stack integration testing
test_monitoring_integration() {
    test_start "Monitoring Stack Integration"
    
    local monitoring_components=("Prometheus" "Grafana")
    local monitoring_urls=("http://localhost:9093" "http://localhost:3000")
    local failed_components=()
    
    # Test Prometheus
    log_debug "Testing Prometheus integration..."
    
    if curl -sf --max-time $TEST_TIMEOUT "${monitoring_urls[0]}/api/v1/query?query=up" > /dev/null; then
        local prometheus_response=$(curl -sf --max-time $TEST_TIMEOUT "${monitoring_urls[0]}/api/v1/query?query=up" 2>/dev/null)
        
        if echo "$prometheus_response" | jq . > /dev/null 2>&1; then
            local active_targets=$(echo "$prometheus_response" | jq -r '.data.result | length')
            
            if [[ $active_targets -gt 0 ]]; then
                log_debug "‚úÖ Prometheus: $active_targets active targets"
            else
                failed_components+=("Prometheus")
                log_debug "‚ùå Prometheus: No active targets"
            fi
        else
            failed_components+=("Prometheus")
            log_debug "‚ùå Prometheus: Invalid response format"
        fi
    else
        failed_components+=("Prometheus")
        log_debug "‚ùå Prometheus: Not accessible"
    fi
    
    # Test Grafana
    log_debug "Testing Grafana integration..."
    
    if curl -sf --max-time $TEST_TIMEOUT "${monitoring_urls[1]}/api/health" > /dev/null; then
        local grafana_response=$(curl -sf --max-time $TEST_TIMEOUT "${monitoring_urls[1]}/api/health" 2>/dev/null)
        
        if echo "$grafana_response" | jq . > /dev/null 2>&1; then
            local grafana_status=$(echo "$grafana_response" | jq -r '.status // "unknown"')
            
            if [[ "$grafana_status" == "ok" ]]; then
                log_debug "‚úÖ Grafana: Healthy"
            else
                failed_components+=("Grafana")
                log_debug "‚ùå Grafana: Status = $grafana_status"
            fi
        else
            failed_components+=("Grafana")
            log_debug "‚ùå Grafana: Invalid response format"
        fi
    else
        failed_components+=("Grafana")
        log_debug "‚ùå Grafana: Not accessible"
    fi
    
    # Evaluate monitoring integration
    if [[ ${#failed_components[@]} -eq 0 ]]; then
        test_pass "Monitoring Stack Integration"
    else
        test_fail "Monitoring Stack Integration" "Failed components: ${failed_components[*]}"
    fi
}

# WebSocket connection testing
test_websocket_connections() {
    test_start "WebSocket Connections"
    
    local websocket_successful=0
    local websocket_total=${#NODES[@]}
    
    for i in "${!NODES[@]}"; do
        local node="${NODES[$i]}"
        local port="${NODE_PORTS[$i]}"
        
        log_debug "Testing WebSocket connection for $node on port $port"
        
        # Basic TCP connection test to WebSocket port
        if timeout 5 bash -c "</dev/tcp/localhost/$port" &> /dev/null; then
            websocket_successful=$((websocket_successful + 1))
            log_debug "‚úÖ $node: WebSocket port accessible"
        else
            log_debug "‚ùå $node: WebSocket port not accessible"
        fi
    done
    
    local websocket_success_rate=$(( (websocket_successful * 100) / websocket_total ))
    
    if [[ $websocket_success_rate -ge 80 ]]; then
        test_pass "WebSocket Connections"
        log_info "WebSocket success rate: $websocket_success_rate% ($websocket_successful/$websocket_total)"
    else
        test_fail "WebSocket Connections" "WebSocket success rate too low: $websocket_success_rate%"
    fi
}

# End-to-end workflow testing
test_end_to_end_workflow() {
    test_start "End-to-End Workflow"
    
    log_debug "Testing complete end-to-end workflow..."
    
    local workflow_steps=(
        "Node connectivity"
        "API accessibility"
        "Blockchain state"
        "Basic transaction"
        "Consensus participation"
        "Monitoring access"
    )
    
    local workflow_successful=true
    local failed_steps=()
    
    # Step 1: Node connectivity
    local connected_nodes=0
    for i in "${!NODES[@]}"; do
        local port="${HEALTH_PORTS[$i]}"
        if curl -sf --max-time $TEST_TIMEOUT "http://localhost:$port/health" > /dev/null; then
            connected_nodes=$((connected_nodes + 1))
        fi
    done
    
    if [[ $connected_nodes -eq ${#NODES[@]} ]]; then
        log_debug "‚úÖ Step 1: All nodes connected"
    else
        workflow_successful=false
        failed_steps+=("Node connectivity")
        log_debug "‚ùå Step 1: Only $connected_nodes/${#NODES[@]} nodes connected"
    fi
    
    # Step 2: API accessibility
    local accessible_apis=0
    for i in "${!NODES[@]}"; do
        local port="${NODE_PORTS[$i]}"
        if curl -sf --max-time $TEST_TIMEOUT "http://localhost:$port/node/info" > /dev/null; then
            accessible_apis=$((accessible_apis + 1))
        fi
    done
    
    if [[ $accessible_apis -eq ${#NODES[@]} ]]; then
        log_debug "‚úÖ Step 2: All APIs accessible"
    else
        workflow_successful=false
        failed_steps+=("API accessibility")
        log_debug "‚ùå Step 2: Only $accessible_apis/${#NODES[@]} APIs accessible"
    fi
    
    # Step 3: Blockchain state consistency
    local consistent_state=true
    local heights=()
    
    for i in "${!NODES[@]}"; do
        local port="${NODE_PORTS[$i]}"
        if curl -sf --max-time $TEST_TIMEOUT "http://localhost:$port/blockchain/height" > /dev/null; then
            local response=$(curl -sf --max-time $TEST_TIMEOUT "http://localhost:$port/blockchain/height" 2>/dev/null)
            if echo "$response" | jq . > /dev/null 2>&1; then
                local height=$(echo "$response" | jq -r '.height // 0')
                heights+=($height)
            else
                consistent_state=false
            fi
        else
            consistent_state=false
        fi
    done
    
    if [[ $consistent_state == true ]]; then
        local min_height=$(printf '%s\n' "${heights[@]}" | sort -n | head -1)
        local max_height=$(printf '%s\n' "${heights[@]}" | sort -n | tail -1)
        local height_diff=$((max_height - min_height))
        
        if [[ $height_diff -le 2 ]]; then
            log_debug "‚úÖ Step 3: Blockchain state consistent"
        else
            workflow_successful=false
            failed_steps+=("Blockchain state")
            log_debug "‚ùå Step 3: Blockchain state inconsistent (height diff: $height_diff)"
        fi
    else
        workflow_successful=false
        failed_steps+=("Blockchain state")
        log_debug "‚ùå Step 3: Cannot verify blockchain state"
    fi
    
    # Step 4: Monitoring access
    if curl -sf --max-time $TEST_TIMEOUT "http://localhost:9093/api/v1/query?query=up" > /dev/null && \
       curl -sf --max-time $TEST_TIMEOUT "http://localhost:3000/api/health" > /dev/null; then
        log_debug "‚úÖ Step 4: Monitoring stack accessible"
    else
        workflow_successful=false
        failed_steps+=("Monitoring access")
        log_debug "‚ùå Step 4: Monitoring stack not accessible"
    fi
    
    # Evaluate end-to-end workflow
    if [[ $workflow_successful == true ]]; then
        test_pass "End-to-End Workflow"
        log_info "All workflow steps completed successfully"
    else
        test_fail "End-to-End Workflow" "Failed steps: ${failed_steps[*]}"
    fi
}

# Generate comprehensive test report
generate_test_report() {
    log_step "Generating comprehensive test report..."
    
    local test_summary="{
        \"test_session_id\": \"integration-tests-${TIMESTAMP}\",
        \"timestamp\": \"$(date -Iseconds)\",
        \"summary\": {
            \"total_tests\": $TESTS_TOTAL,
            \"passed\": $TESTS_PASSED,
            \"failed\": $TESTS_FAILED,
            \"skipped\": $TESTS_SKIPPED,
            \"success_rate\": $(( TESTS_TOTAL > 0 ? (TESTS_PASSED * 100) / TESTS_TOTAL : 0 ))
        },
        \"environment\": {
            \"nodes\": ${#NODES[@]},
            \"node_ports\": [$(IFS=','; echo "${NODE_PORTS[*]}")],
            \"health_ports\": [$(IFS=','; echo "${HEALTH_PORTS[*]}")],
            \"test_duration\": \"$(($(date +%s) - $(stat -c %Y "$TEST_LOG")))s\"
        },
        \"status\": \"$([ $TESTS_FAILED -eq 0 ] && echo 'success' || echo 'failed')\",
        \"recommendations\": []
    }"
    
    # Add recommendations based on test results
    local recommendations="[]"
    
    if [[ $TESTS_FAILED -gt 0 ]]; then
        local failure_recommendation="{
            \"priority\": \"high\",
            \"type\": \"test_failures\",
            \"description\": \"$TESTS_FAILED integration tests failed. Review failed tests and address issues.\",
            \"action\": \"Check test logs for detailed failure information and fix underlying issues\"
        }"
        recommendations=$(echo "$recommendations" | jq ". += [$failure_recommendation]")
    fi
    
    if [[ $TESTS_SKIPPED -gt 0 ]]; then
        local skip_recommendation="{
            \"priority\": \"medium\",
            \"type\": \"skipped_tests\",
            \"description\": \"$TESTS_SKIPPED tests were skipped. Verify if this is expected.\",
            \"action\": \"Review skipped tests and ensure required features are properly configured\"
        }"
        recommendations=$(echo "$recommendations" | jq ". += [$skip_recommendation]")
    fi
    
    local success_rate=$(( TESTS_TOTAL > 0 ? (TESTS_PASSED * 100) / TESTS_TOTAL : 0 ))
    
    if [[ $success_rate -lt 90 ]]; then
        local performance_recommendation="{
            \"priority\": \"medium\",
            \"type\": \"test_performance\",
            \"description\": \"Test success rate is $success_rate%, below recommended 90%.\",
            \"action\": \"Investigate system performance and stability issues\"
        }"
        recommendations=$(echo "$recommendations" | jq ". += [$performance_recommendation]")
    fi
    
    # Update test summary with recommendations
    test_summary=$(echo "$test_summary" | jq ".recommendations = $recommendations")
    
    # Write test report
    echo "$test_summary" | jq '.' > "$TEST_REPORT"
    
    log_success "Test report generated: $TEST_REPORT"
    
    # Display final summary
    log_info "=== INTEGRATION TEST SUMMARY ==="
    log_info "Test Session: integration-tests-${TIMESTAMP}"
    log_info "Total Tests: $TESTS_TOTAL"
    log_info "Passed: $TESTS_PASSED"
    log_info "Failed: $TESTS_FAILED"
    log_info "Skipped: $TESTS_SKIPPED"
    log_info "Success Rate: $success_rate%"
    log_info "Status: $([ $TESTS_FAILED -eq 0 ] && echo 'SUCCESS' || echo 'FAILED')"
    log_info "============================="
}

# Main integration testing function
main() {
    echo -e "${CYAN}üß™ DYTALLIX TESTNET INTEGRATION TEST SUITE${NC}"
    echo -e "${CYAN}===========================================${NC}"
    echo
    
    log_info "Starting comprehensive integration testing..."
    log_info "Test session: integration-tests-${TIMESTAMP}"
    log_info "Test log: $TEST_LOG"
    echo
    
    # Setup test environment
    setup_test_environment
    echo
    
    # Run all integration tests
    test_node_connectivity
    echo
    
    test_api_endpoints
    echo
    
    test_blockchain_consistency
    echo
    
    test_smart_contract_deployment
    echo
    
    test_transaction_processing
    echo
    
    test_consensus_mechanism
    echo
    
    test_block_production
    echo
    
    test_websocket_connections
    echo
    
    test_monitoring_integration
    echo
    
    if [[ "${LOAD_TEST:-false}" == "true" ]]; then
        test_performance_under_load
        echo
    fi
    
    test_end_to_end_workflow
    echo
    
    # Generate comprehensive report
    generate_test_report
    echo
    
    if [[ $TESTS_FAILED -eq 0 ]]; then
        log_success "üéâ All integration tests passed successfully!"
        log_info "Testnet is ready for production use"
        exit 0
    else
        log_error "‚ùå $TESTS_FAILED integration tests failed"
        log_info "Review test logs and address issues before production deployment"
        exit 1
    fi
}

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --debug)
            DEBUG=true
            shift
            ;;
        --load-test)
            LOAD_TEST=true
            shift
            ;;
        --timeout)
            TEST_TIMEOUT="$2"
            shift 2
            ;;
        --help|-h)
            echo "Dytallix Testnet Integration Test Suite"
            echo ""
            echo "Usage: $0 [OPTIONS]"
            echo ""
            echo "Options:"
            echo "  --debug       Enable debug logging"
            echo "  --load-test   Include performance load testing"
            echo "  --timeout N   Set test timeout in seconds (default: $TEST_TIMEOUT)"
            echo "  --help        Show this help message"
            echo ""
            echo "Environment Variables:"
            echo "  DEBUG         Enable debug logging (true/false)"
            echo "  LOAD_TEST     Enable load testing (true/false)"
            echo ""
            echo "Test Categories:"
            echo "  - Node connectivity and health checks"
            echo "  - API endpoint comprehensive testing"
            echo "  - Blockchain state consistency"
            echo "  - Smart contract deployment capability"
            echo "  - Transaction processing validation"
            echo "  - Consensus mechanism verification"
            echo "  - Block production testing"
            echo "  - WebSocket connection testing"
            echo "  - Monitoring stack integration"
            echo "  - Performance testing under load"
            echo "  - End-to-end workflow validation"
            echo ""
            exit 0
            ;;
        *)
            log_error "Unknown option: $1"
            echo "Use --help for usage information"
            exit 1
            ;;
    esac
done

# Ensure directories exist
mkdir -p "$LOG_DIR"
mkdir -p "$TEST_RESULTS_DIR"

# Run main function
main